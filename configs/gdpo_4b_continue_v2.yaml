# GDPO Training Config: Continue from checkpoint on v2 graph seeds
#
# Uses the published GDPO checkpoint as the starting policy and trains on
# the v2 multi-objective attack-graph seeds.

model: Jarrodbarnes/opensec-gdpo-4b
manifest: data/seeds/manifest.json
output_dir: outputs/gdpo-4b-v2

# Episode settings
max_steps: 15

# GDPO-specific settings
use_gdpo: true
reward_weights: null

# Batch and rollout settings (single A100 with attacker SGLang)
num_generations: 16
num_prompts: 64
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
max_completion_length: 1024

# SGLang inference server for fast generation
inference_url: "http://localhost:8001"

# Epoch-level on-policy: reload SGLang weights after each epoch
policy_reload_cmd: scripts/reload_sglang.sh

# Gradient checkpointing
gradient_checkpointing: true

# Learning rate: keep prior scale for continuity with the published run
learning_rate: 2.0e-6

# Curriculum settings
curriculum: false
epochs_per_stage: 12
use_eval_curriculum: false

# Per-tier KL / entropy configs
tier_kl_config:
  trivial: 0.01
  easy: 0.005
  standard: 0.0

tier_entropy_config:
  trivial: 0.01
  easy: 0.005
  standard: 0.0

# Disable KL penalty (no ref model)
beta: 0.0

# Use standard GRPO loss
loss_type: "grpo"

# Logging and checkpoints
logging_steps: 10
save_strategy: epoch

# Replay cache
replay_cache: data/sqlite/replay_cache.db
