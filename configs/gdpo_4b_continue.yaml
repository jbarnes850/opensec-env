# GDPO Training Config: Continue from checkpoint
#
# Continues training from epoch 2 checkpoint for +4 epochs (total 6)
# For incremental evaluation: check epochs 4 and 6

# Start from trained checkpoint (not base model)
model: outputs/gdpo-4b/checkpoint-standard
manifest: data/seeds/manifest.json
output_dir: outputs/gdpo-4b-epoch6

# Episode settings
max_steps: 15

# GDPO-specific settings
use_gdpo: true
reward_weights: null

# Batch and rollout settings (same as before)
num_generations: 8
num_prompts: 16
per_device_train_batch_size: 8
gradient_accumulation_steps: 8
max_completion_length: 1024

# SGLang inference server (will be restarted with checkpoint)
inference_url: "http://localhost:8001"

# Gradient checkpointing
gradient_checkpointing: true

# Learning rate: same as before for consistency
learning_rate: 2.0e-6

# Run 4 more epochs (flat curriculum since no tiered data)
curriculum: true
epochs_per_stage: 4
use_eval_curriculum: false

# Disable KL penalty
beta: 0.0

# Per-tier configs (not used with flat curriculum but kept for compatibility)
tier_kl_config:
  trivial: 0.01
  easy: 0.005
  standard: 0.0

tier_entropy_config:
  trivial: 0.01
  easy: 0.005
  standard: 0.0

# Use standard GRPO loss
loss_type: "grpo"

# Logging and checkpoints - save every epoch for evaluation
logging_steps: 10
save_strategy: epoch

# Replay cache
replay_cache: data/sqlite/replay_cache.db
